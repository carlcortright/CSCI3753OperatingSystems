\documentclass{article}
\title{Analysis of Linux Schedulers}
\author{Carl Cortright}
\date{December 3, 2016}
\usepackage{listings}
\usepackage{graphicx}

\begin{document}
\maketitle

\section{Abstract}

\section{Introduction}

Operating system schedulers are systems that are unseen, but they play a huge role in every day life, helping run our everyday computers so they don’t appear to freeze, preventing fatal scheduling bugs in our financial systems, even ensuring that life-critical systems on aircraft continue functioning. Different operating schedulers perform differently depending on the operations they are scheduling. I will start by designing an experiment to evaluate three different schedulers, the standard Linux completely fair scheduler, real time scheduler, and the naive first in first out scheduler. Testing the schedulers under varying processor utilizations, priorities of children, and IO vs CPU operations, I ran extensive tests of the performance of each scheduler. I then evaluate these systems on their performance using standard benchmarks, including how much system, user, and real time they take on the processor. Lastly, I compare the systems to each other, analyzing their advantages/disadvantages in any given situation.

\section{Experimental Design}

To properly evaluate the performance of operating schedulers, it is important to have a proper experimental design that minimizes outside factors including other processes that might produce uneven results over time. For standardization purposes, I performed this experiment on a Dell Inspiron 5520 with a 4-core Intel i7 processor, 8GB (DDR3) of memory, and upgraded Samsung SSD (read/write up to 520mb/sec). At the time of the experiment, the machine was also running a Deep-Q learning program that had been running for days. The Deep-Q Learning program was consistently using approximately 500MB of RAM along with 25 percent processor utilization. After multiple runs of the test program, I was able to determine that this program had no noticeable effect on the results of the experiment.

To test the performance of each scheduler, I wrote three different c programs, one to test the schedulers under CPU intensive tasks, one to test the schedulers under IO intensive task, and one to test both at the same time. Each of these C programs had different arguments that could test different configurations; the CPU program was statistically calculating pi and could specify the number of children and iterations of the pi calculator, along with scheduling policy and if priority should be varied between children; the IO program was writing and reading a file and could specify number of processes, policy, if priority should be varied, transfer size, and block size; the mixed program calculated pi before doing read/write operations and it could specify number of children, policy, varied process priority, transfer size, block size, and iterations. Usage for each of the three programs is described below:

\begin{lstlisting}[language=bash]
  $./pi-sched <children> <iterations> <policy>
  <vary priority>
  $./rw-sched <number of processes> <policy>
  <vary priority> <transfer size> <block size>
  $./mixed-sched <number of processes> <policy>
  <vary priority> <transfer size> <block size> <iterations>
\end{lstlisting}

Each of the programs forks the number of processes specified and does the IO or CPU intensive operations. I used the c resource library’s getrusage() command to get relevant user and system time usages for the children processes. Separately I used the time library’s clockgettime() command to get real time data for how long each run took to execute. For the programs that involved reading and writing, it was important that each child process had its own set of input and output files to work with, because if they were all reading and writing to the same file, there would be a clear bottleneck that would skew our experiments results. Before I started the timers, I wrote data from /dev/random to separate input files for each process. It was this process that made using time time utility unpractical for the experiment. The time utility would have included these operations in its calculation, and in order to get accurate results for the experiment it was important to only time the child processes.

To get consistent results for the CPU and IO intensive operations, I picked fixed values for iterations, transfer size, and block size. For the the number of iterations to calculate Pi, I fixed the value at 10 million iterations. For the IO operations, I picked a fixed transfer size of 1KB (1024 Bytes) with an 8 Byte block size.

To get a wide range of results, I wrote a bash script that ran each of the three programs 18 times. This amounted to 54 test cases. For each program, I tested each scheduler over varying priorities and number of child processes. Each scheduler was tested over three cases, when it had 5 children, 50 children, and 500 children, without varying priority. Then, the same tests were repeated, except this time priority was varied. To make it easier to analyze, each of the test programs outputted its configuration along with results in the standard CSV format, making it easy to process the data in a standard Google Sheet.

\section{Results}

When analyzing the results of my experiments, it was important to recognize the metrics that best evaluated the performance of each scheduler. I chose to use “real time” as my main metric of evaluation, along with real time per process, calculated after the experiment. In my analysis, I will look some at the effects CPU or IO has on system and user time, but the focus will be on the real time metrics. 

\subsection{CPU Task Results}
To assess the performance of the different schedulers against each other, I took the data collected and graphed it using bar graphs, once with varied priorities, and then again without varied priorities. I used the trial with 500 processes for each so that the distinction would be more clear:

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{CPUNotVaried.png}
  \caption{Running times of CPU tasks on different schedulers, 500 child processes without varying priority}
\end{figure}

I then did the same with varied priorities. Notice for later that the completely fair scheduler has a significantly faster running time than the other two which got marginally slower. For reference, it has a time per process of only 0.012 seconds which was the fastest of any of the trials. Because the completely fair scheduler had such remarkable performance, I decided to dive a bit deeper and plot a line graph of the Time per Process vs. Number of Processes.

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{CPUVaried.png}
  \caption{Running times of CPU tasks on different schedulers, 500 child processes with varied priority}
\end{figure}


\begin{figure}[h!]
  \includegraphics[width=\linewidth]{CPUProcessesGraph.png}
  \caption{Improvement of running time per process over time with the varied completely fair scheduler}
\end{figure}


\subsection{IO Task Results}

To allow for accurate comparisons, I graphed the same trials as the CPU task section. Again, I graphed it using bar graphs, once with varied priorities and once without varied priorities, using 500 processes.


\begin{figure}[h!]
  \includegraphics[width=\linewidth]{IONotVaried.png}
  \caption{Running times of IO tasks on different schedulers, 500 child processes without varying priority}
\end{figure}

I then did the same for varied priorities, plotting the running times for the 500 process trials on a bar graph. Notice the continued pattern of the completely fair scheduler maintaining its speed even as the number of processes increases by an order of magnitude.


\begin{figure}[h!]
  \includegraphics[width=\linewidth]{IOVaried.png}
  \caption{Running times of IO tasks on different schedulers, 500 child processes with varied priority}
\end{figure}

Another thing to notice with the IO processes is that the trials that had fewer processes were dramatically slower per process. I also included this histogram.


\begin{figure}[h!]
  \includegraphics[width=\linewidth]{IOAverageRuntime.png}
  \caption{Average runtime of a process for IO operations}
\end{figure}

\subsection{Mixed Task Results}

I produced the same graphs for the mixed tasks. The results were very similar to what would be expected from adding the IO and CPU tasks together. All of the data for my results section is listed in Appendix A.


\begin{figure}[h!]
  \includegraphics[width=\linewidth]{MixedNotVaried.png}
  \caption{Running times of Mixed tasks on different schedulers, 500 child processes without varied priority}
\end{figure}


\begin{figure}[h!]
  \includegraphics[width=\linewidth]{MixedVaried.png}
  \caption{Running times of Mixed tasks on different schedulers, 500 child processes with varied priority}
\end{figure}

\section{Analysis}



\section{Analysis}

\section{Conclusion}

\section{Appendix A: Data}

\section{Appendix B: Code}


\end{document}
